{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#basics\n",
    "import sys\n",
    "import scipy\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#scaling methods\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "#neurokit\n",
    "#import neurokit as nk\n",
    "#import seaborn as sns\n",
    "\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "#models\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#fourier transform\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "from sklearn.decomposition import (PCA, LatentDirichletAllocation)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "# Importing metrics for evaluation\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "\n",
    "#biospy\n",
    "import random as rn\n",
    "from biosppy.signals import (ecg, tools)\n",
    "import pywt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import math\n",
    "from itertools import product\n",
    "# ============= CONSTS =============\n",
    "#TRAIN_FILE_PATH = \"/X_train.csv\"\n",
    "#TARGET_FILE_PATH =  \"/y_train.csv\"\n",
    "#TEST_FILE_PATH = \"/X_test.csv\"\n",
    "\n",
    "SEED=42\n",
    "NUM_MAX_POINTS = 18154\n",
    "\n",
    "#this is the frequency I guess?\n",
    "SAMPLING_RATE=300\n",
    "USE_WAVE_LETS = False\n",
    "my_cols = [\"id\"] + [\"x\" + str(i) for i in range(NUM_MAX_POINTS)]\n",
    "# ============= CONSTS =============\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "\n",
    "\n",
    "# svc parameters\n",
    "SVC_C = 1.0\n",
    "SVC_KERNEL = 'rbf'\n",
    "SVC_K = 1000\n",
    "def SVC_GAMMA(X, f, k):\n",
    "    return 1/(k*f(X))\n",
    "\n",
    "# gradient boost parameters\n",
    "GBC_LOSS = 'deviance'\n",
    "GBC_L_RATE = 0.098\n",
    "GBC_N_ESTIMATORS = 150\n",
    "GBC_MAX_DEPTH = 4\n",
    "\n",
    "# xgb parameters\n",
    "XGB_OBJECTIVE = 'multi:softprob'\n",
    "XGB_L_RATE = 0.2\n",
    "XGB_N_ESTIMATORS = 150\n",
    "XGB_BOOSTER = 'gbtree'\n",
    "XGB_MAX_DEPTH = 6\n",
    "XGB_LAMBDA = 0.0\n",
    "XGB_ALPHA = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = pd.read_csv('/Users/charlotteout/Documents/AML/task3/X_train.csv', index_col=0)\n",
    "\n",
    "\n",
    "Y_train = pd.read_csv('/Users/charlotteout/Documents/AML/task3/y_train.csv', index_col=0)\n",
    "\n",
    "\n",
    "X_test_data =  pd.read_csv('/Users/charlotteout/Documents/AML/task3/X_test.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(signal, sampling_rate):\n",
    "    \n",
    "    X = list()\n",
    "    ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts,heart_rate = ecg.ecg(signal, sampling_rate, show=False)\n",
    "    \n",
    "    templates1, rpeaks1 = ecg.extract_heartbeats(filtered, rpeaks, sampling_rate)\n",
    "    \n",
    "    rpeaks2 = ecg.correct_rpeaks(signal=signal, rpeaks=rpeaks, sampling_rate=sampling_rate, tol=0.1)\n",
    "    \n",
    "    rpeaksamps = np.take(filtered, rpeaks)\n",
    "    \n",
    "    if len(heart_rate) < 2:\n",
    "        heart_rate = [0,1]\n",
    "    if len(heart_rate_ts) < 2:\n",
    "        heart_rate_ts = [0,1]\n",
    "    \n",
    "    #rpeaksamps stats\n",
    "    X.append(np.mean(rpeaksamps))\n",
    "    X.append(np.min(rpeaksamps))\n",
    "    X.append(np.max(rpeaksamps))\n",
    "    X.append(np.std(rpeaksamps))\n",
    "    X.append(np.median(rpeaksamps))\n",
    "    \n",
    "    X.append(np.mean(np.diff(rpeaksamps)))\n",
    "    X.append(np.min(np.diff(rpeaksamps)))\n",
    "    X.append(np.max(np.diff(rpeaksamps)))\n",
    "    X.append(np.std(np.diff(rpeaksamps)))\n",
    "    X.append(np.median(np.diff(rpeaksamps)))\n",
    "    \n",
    "    #heart rate stats\n",
    "    X.append(np.mean(heart_rate))\n",
    "    X.append(np.std(heart_rate))\n",
    "    X.append(np.min(heart_rate))\n",
    "    X.append(np.max(heart_rate))\n",
    "    X.append(np.median(heart_rate))\n",
    "    \n",
    "    X.append(np.mean(np.diff(heart_rate)))\n",
    "    X.append(np.min(np.diff(heart_rate)))\n",
    "    X.append(np.max(np.diff(heart_rate)))\n",
    "    X.append(np.std(np.diff(heart_rate)))\n",
    "    X.append(np.median(np.diff(heart_rate)))\n",
    "    \n",
    "    #statistics of R-R interval\n",
    "    \n",
    "    RR_int =[]\n",
    "    T = 1/300\n",
    "    \n",
    "    for k in range(1,len(rpeaks1)):\n",
    "        RR_int.append((ts[rpeaks[k]] - ts[rpeaks[k-1]]))\n",
    "    if len(RR_int) != 0:\n",
    "        X.append(np.mean(RR_int))\n",
    "        X.append(np.std(RR_int))\n",
    "        X.append(np.min(RR_int))\n",
    "        X.append(np.max(RR_int))\n",
    "        X.append(np.median(RR_int))\n",
    "        \n",
    "    X.append(60000 / np.mean(RR_int))\n",
    "    \n",
    "    X += list(np.mean(templates1, axis=0))\n",
    "    X += list(np.min(templates1, axis=0))\n",
    "    X += list(np.max(templates1, axis=0))\n",
    "    X += list(np.std(templates1, axis=0))\n",
    "    X += list(np.median(templates1, axis=0))\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    X[np.isnan(X)] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276ad91bd80b4907911e294f7caff793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5117, 926)\n"
     ]
    }
   ],
   "source": [
    "features_train = list()\n",
    "sampling_rate = float(SAMPLING_RATE)\n",
    "for id in tqdm(range(X_train_data.shape[0])):\n",
    "    #dropping the NaN's in this way\n",
    "    signal = np.array(pd.to_numeric(X_train_data.iloc[id].dropna()))\n",
    "    features_train.append(get_features(signal, sampling_rate))\n",
    "    \n",
    "    \n",
    "X_train = np.array(features_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8ba43eaea644faac0829529348c512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3411), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(3411, 926)\n"
     ]
    }
   ],
   "source": [
    "features_test = list()\n",
    "for id in tqdm(range(X_test_data.shape[0])):\n",
    "    signal = np.array(pd.to_numeric(X_test_data.iloc[id].dropna()))\n",
    "    features_test.append(get_features(signal,sampling_rate))\n",
    "    \n",
    "X_test = np.array(features_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling methods \n",
    "\n",
    "def Standardscaler(data):\n",
    "    scaler = StandardScaler()\n",
    "    fitscal = scaler.fit(data)\n",
    "    return fitscal\n",
    "\n",
    "def PowTrans(data):\n",
    "    scaler = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "    fitscal = scaler.fit(data)\n",
    "    return fitscal\n",
    "\n",
    "def MinMax(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    fitscal = scaler.fit(data)\n",
    "    return fitscal\n",
    "\n",
    "def Quantile(data):\n",
    "    scaler = QuantileTransformer()\n",
    "    fitscal = scaler.fit(data)\n",
    "    return fitscal\n",
    "\n",
    "\n",
    "def Robust(data):\n",
    "    scaler = RobustScaler()\n",
    "    fitscal = scaler.fit(data)\n",
    "    return fitscal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELS\n",
    "\n",
    "#naive bayes\n",
    "def NB(data_set_X, data_set_y):\n",
    "    model = GaussianNB()\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "# svm classifier model\n",
    "def svc(data_set_X, data_set_y):\n",
    "    model = SVC(random_state=seed)\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# mlp classifier model\n",
    "def mlp(data_set_X, data_set_y):\n",
    "    model = MLPClassifier(random_state=0)\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "# ada boost model\n",
    "def ada_boost(data_set_X, data_set_y):\n",
    "    model = AdaBoostClassifier()\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "# random forest classifier model\n",
    "def rfc(data_set_X, data_set_y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "# quadratic discriminant analysis model\n",
    "def qda(data_set_X, data_set_y):\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "def GradBoost(data_set_X, data_set_y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(data_set_X, data_set_y)\n",
    "    return model\n",
    "\n",
    "def xgbc(data_set_X, data_set_y):\n",
    "    model = XGBClassifier(objective=XGB_OBJECTIVE, learning_rate=XGB_L_RATE,\n",
    "                         n_estimators=XGB_N_ESTIMATORS, booster=XGB_BOOSTER,\n",
    "                         max_depth=XGB_MAX_DEPTH,reg_lambda=XGB_LAMBDA,\n",
    "                         reg_alpha=XGB_ALPHA, random_state=SEED)\n",
    "    model.fit(data_set_X,data_set_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kfold(classifier, X, y, k):\n",
    "    kf = KFold(n_splits=k)\n",
    "\n",
    "    mses_eval = []\n",
    "   \n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test, y_train = X[train_index], X[test_index], y[train_index]\n",
    "        scaler = Robust(X_train)\n",
    "        #X_train = scaler.transform(X_train)\n",
    "        #X_test = scaler.transform(X_test)\n",
    "        # X_train, y_train = resample(X_train, y_train)\n",
    "        #X_train, X_test = do_best_select(, X_train, X_test, y_train)\n",
    "        model = classifier(X_train, y_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        mses_eval.append(f1_score(y[test_index], y_pred_test, average=\"micro\"))\n",
    "       \n",
    "        print(mses_eval)\n",
    "       \n",
    "\n",
    "    return np.mean(mses_eval), np.std(mses_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ravel(np.array(Y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8037109375]\n",
      "[0.8037109375, 0.810546875]\n",
      "[0.8037109375, 0.810546875, 0.8240469208211144]\n",
      "[0.8037109375, 0.810546875, 0.8240469208211144, 0.7869012707722385]\n",
      "[0.8037109375, 0.810546875, 0.8240469208211144, 0.7869012707722385, 0.8035190615835777]\n"
     ]
    }
   ],
   "source": [
    "mean, std1 = do_kfold(GradBoost,X_train, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8057450131353863\n",
      "0.01201829149716305\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "#split train and test data in 80 20 \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scaling\n",
    "myScale = Robust(X_train1)\n",
    "X_train1 = myScale.transform(X_train1)\n",
    "X_test1 = myScale.transform(X_test1)\n",
    "\n",
    "\n",
    "\n",
    "#Model\n",
    "model = xgbc(X_train1, y_train1)\n",
    "y_pred_test = model.predict(X_test1)\n",
    "\n",
    "\n",
    "print(f1_score(y_test1, y_pred_test, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Write predictions into csv file\n",
    "'''\n",
    "\n",
    "# function to write csv file\n",
    "def csv_write(prediction):\n",
    "\n",
    "    # size of prediction\n",
    "    n_size = prediction.size\n",
    "\n",
    "    # header\n",
    "    header = []\n",
    "    header.append('id')\n",
    "    header.append('y')\n",
    "\n",
    "    # array containing ids\n",
    "    ids = []\n",
    "\n",
    "    for i in range(0, n_size):\n",
    "        ids.append(float(i))\n",
    "\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    # write file\n",
    "    with open('prediction3_sixth.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = ['id', 'y'])\n",
    "        writer.writeheader()\n",
    "        nsize = prediction.size\n",
    "        for i in range(0, nsize):\n",
    "            row = {}\n",
    "            row['id'] = ids[i]\n",
    "            row['y'] = prediction[i]\n",
    "            writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= Robust(X_train)\n",
    "X_train_data_1 = scaler.transform(X_train)\n",
    "X_test_data_1 =scaler.transform(X_test)\n",
    "\n",
    "model = xgbc(X_train_data_1, y)\n",
    "y_pred_test = model.predict(X_test_data_1)\n",
    "\n",
    "csv_write(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
